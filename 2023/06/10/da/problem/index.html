<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 6.3.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"example.com","root":"/","scheme":"Gemini","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta name="description" content="分布近似问题简介。">
<meta property="og:type" content="article">
<meta property="og:title" content="采样与生成（1）：分布近似问题">
<meta property="og:url" content="http://example.com/2023/06/10/da/problem/index.html">
<meta property="og:site_name" content="Some notes">
<meta property="og:description" content="分布近似问题简介。">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="http://example.com/2023/06/10/da/problem/uncertainty.png">
<meta property="og:image" content="http://example.com/2023/06/10/da/problem/parvi.png">
<meta property="og:image" content="http://example.com/2023/06/10/da/problem/flow.png">
<meta property="article:published_time" content="2023-06-10T14:13:00.000Z">
<meta property="article:modified_time" content="2023-10-01T13:02:19.025Z">
<meta property="article:author" content="Li zhijian">
<meta property="article:tag" content="机器学习">
<meta property="article:tag" content="贝叶斯学习">
<meta property="article:tag" content="分布近似">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://example.com/2023/06/10/da/problem/uncertainty.png">

<link rel="canonical" href="http://example.com/2023/06/10/da/problem/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'en'
  };
</script>

  <title>采样与生成（1）：分布近似问题 | Some notes</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Some notes</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2023/06/10/da/problem/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Li zhijian">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Some notes">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          采样与生成（1）：分布近似问题
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2023-06-10 22:13:00" itemprop="dateCreated datePublished" datetime="2023-06-10T22:13:00+08:00">2023-06-10</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2023-10-01 21:02:19" itemprop="dateModified" datetime="2023-10-01T21:02:19+08:00">2023-10-01</time>
              </span>

          
            <div class="post-description">分布近似问题简介。</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <h2 id="分布近似问题">分布近似问题</h2>
<p>就本人目前所了解的现状，分布近似问题尚未在学界被严肃定义，多数研究者可能熟悉的是多年来持续火热的生成模型：生成对抗网络（Generative
Adversarial Networks, GAN），扩散模型（Diffusion
Model）等等。另一些核心却冷门的领域，如马尔科夫链蒙特卡洛采样（Markov
Chain Monte Carlo, MCMC），或是变分推理（Variational Inference,
VI），则极少成为人们热议的对象。实际上，MCMC与VI常常出现在生成模型的论文中，从远古时期的变分自编码器VAE，到Energy-based、基于Score
Matching的生成模型，再到近两年火热的扩散模型（Diffusion
Model），甚至生成对抗网络（GAN）都可以被解释为使用特殊kernel的VI。另一方面，起始于2016年的粒子变分推理法（Particle-based
Variational Inference,
ParVI）被逐步构建与MCMC、VI方法的联系，生成与采样逐渐被统一理解为求解分布近似问题。</p>
<h3 id="问题定义">问题定义</h3>
<p>什么是分布近似问题呢？该问题可被定义为如下形式：</p>
<p><span class="math display">\[
\min_{\mu} \mathcal{F}(\mu, \nu), \tag{1}
\]</span></p>
<p>这里的<span class="math inline">\(\mu\)</span>和<span class="math inline">\(\nu\)</span>表示概率分布，<span class="math inline">\(\mathcal{F}\)</span>表示一个度量，计算分布<span class="math inline">\(\mu\)</span>和<span class="math inline">\(\nu\)</span>之间的差异（不一定是距离），比如常见的KL散度（Kullback-Leibler
Divergence），不常见的最大平均差异（Maximum Mean Discrepancy,
MMD）和沃瑟斯坦距离（Wasserstein Distance）。</p>
<p>公式<span class="math inline">\((1)\)</span>定义了一个优化问题，优化目标是<span class="math inline">\(\mathcal{F}(\cdot, \nu)\)</span>，或者写成<span class="math inline">\(\mathcal{F}_{\nu}\)</span>，优化变量是一个分布<span class="math inline">\(\mu\)</span> ， 全局最优为另一个分布 <span class="math inline">\(\nu\)</span> 。也就是说，我们希望找到一个分布<span class="math inline">\(\mu\)</span>在度量<span class="math inline">\(\mathcal{F}\)</span>为判别标准的情况下接近分布<span class="math inline">\(\nu\)</span>。</p>
<p>看起来，这个形式<span class="math inline">\((1)\)</span>非常接近我们熟知的实向量空间中的优化（熟悉凸优化的读者应该能很快get到联系）：</p>
<p><span class="math display">\[
\min_{x} F(x). \tag{2}
\]</span></p>
<p>比如在深度学习领域，优化目标<span class="math inline">\(F\)</span>表示训练的损失函数, 优化变量<span class="math inline">\(x\)</span>表示神经网络的参数。当然，神经网络的参数是逐层的，但是我们可以将其抽象表示为一个高纬度向量。此外，由于训练过程中输入的数据（集）不计算梯度，所以可以不把这些变量表示在公式<span class="math inline">\((2)\)</span>中。实际上在上述的表达中，数据集和网络结构共同定义了优化目标<span class="math inline">\(F\)</span>。</p>
<p>看起来，公式<span class="math inline">\((1)\)</span>可类比作公式<span class="math inline">\((2)\)</span>的“升级”形式:</p>
<ul>
<li>优化目标<span class="math inline">\(F\)</span>计算两个<strong>向量</strong>之间的差异<span class="math inline">\(\to\)</span>优化目标<span class="math inline">\(\mathcal{F}\)</span>计算两个<strong>分布</strong>之间的差异。</li>
<li>优化变量<span class="math inline">\(x\)</span>是一个<strong>实向量</strong><span class="math inline">\(\to\)</span>优化变量<span class="math inline">\(\mu\)</span>是一个<strong>概率分布</strong>。</li>
</ul>
<h3 id="贝叶斯学习">贝叶斯学习</h3>
<blockquote>
<p>那么表达式<span class="math inline">\((1)\)</span>也可以按照上述神经网络的方式来理解吗？</p>
</blockquote>
<p>实际上，如果从训练神经网络的角度阐释<span class="math inline">\((1)\)</span>，
我们可以非常方便地理解<strong>贝叶斯学习</strong>。首先回顾贝叶斯学习的基础——贝叶斯公式，建模了后验、似然、先验之间的关系：</p>
<p><span class="math display">\[
p(x|y) = \frac{p(y|x)p(x)}{p(y)}.
\]</span></p>
<p>假设现在使用贝叶斯公式建模一个分类任务的神经网络参数<span class="math inline">\(x\)</span>的后验分布，则有：</p>
<p><span class="math display">\[
p(x|\mathcal{D}_{train}) =
\frac{p(\mathcal{D}_{train}|x)p(x)}{p(\mathcal{D}_{train})},
\]</span></p>
<p>这里使用<span class="math inline">\(\mathcal{D}_{train} = \{d_{train,
j}\}^N_{j=1}\)</span>表示训练数据集（为了表达简洁就不再拆分为输入和输出了），上述公式可进一步表达为：</p>
<p><span class="math display">\[
p(x| \mathcal{D}_{train} ) \propto \prod^N_{j=1}{p( d_{train, j}
|x)}p(x), \tag{3}
\]</span></p>
<p>上式表达了在给定训练数据集的情况下，神经网络参数的后验<strong>分布</strong>，可以被建模为<strong>每一条输入数据的似然函数的乘积</strong>，再乘上模型参数的先验分布<span class="math inline">\(p(x)\)</span>。通常似然函数的形式比较复杂（由神经网络决定），先验分布形式简单，例如高斯分布。再推导一步，可得：</p>
<p><span class="math display">\[
-\log{p(x|\mathcal{D}_{train})} = -\left(\sum^{N}_{j=1}\log{p(d_{train,
j}|x)} + \log{p(x)}\right).
\]</span></p>
<p>显然，等式右边就是常见的分类任务的损失函数，其中加号左边可以是<code>BCE</code>、<code>CE</code>这种，加号右边可以是<code>L1</code>、<code>L2</code>正则化。如果按照优化的思路，找到一个神经网络的参数<span class="math inline">\(x\)</span>使得 <span class="math inline">\(-\log{p(x|\mathcal{D}_{train})}\)</span>
最小，我们就找到了当前训练集上的一个最优模型，神经网络的推理过程就是再次计算似然函数：</p>
<p><span class="math display">\[
p(d_{test}|\mathcal{D}_{train}) = p(d_{test}|x^*), \quad \text{where}
\quad x^* = \arg\min_{x} -\log{p(x|\mathcal{D}_{train})}. \tag{4}
\]</span></p>
<p><strong>然而对于贝叶斯学习，或者说贝叶斯推理，推理过程被建模为</strong>：</p>
<p><span class="math display">\[
\begin{align*}
    \begin{split}
    p(d_{test}|\mathcal{D}_{train})
    &amp; = \int \underbrace{p(d_{test}| x)}_{prediction}
\frac{\overbrace{p(\mathcal{D}_{train}|x)}^{likelihood}\overbrace{p(x)}^{prior}}{\underbrace{p(\mathcal{D}_{train})}_{marginal}}\mathrm{d}x
\\
    &amp; = \int
\underbrace{p(d_{test}|x)}_{prediction}\underbrace{p(x|\mathcal{D}_{train})}_{posterior}\mathrm{d}x
\\
    &amp; = \mathbb{E}_{x \sim
p(x|\mathcal{D}_{train})}\left[p(d_{test}|x)\right].
    \end{split} \tag{5}
\end{align*}
\]</span></p>
<p>对比式<span class="math inline">\((4)\)</span>和<span class="math inline">\((5)\)</span>，可以发现前者取当前训练集的最优的<strong>一个</strong>模型推理输出概率，而后者计算输出概率关于模型后验分布<span class="math inline">\(\mathbb{E}_{x \sim
p(x|\mathcal{D}_{train})}\)</span>的期望（当然也可以计算方差，建模输出结果的不确定度）。</p>
<p>这就是很多论文提到的：</p>
<ol type="1">
<li>贝叶斯学习（推理）体现了模型输出的不确定度：单个预测结果<span class="math inline">\(\to\)</span>预测结果的分布</li>
<li>贝叶斯学习在少样本情况下不容易过拟合（此时贝叶斯学习的行为类似于集成学习，但一个显著的区别是，若损失函数的surface是凸的，那么集成学习的所有优化点都会坍缩到全局最优，但是贝叶斯学习的分布不会坍缩为狄拉克分布）。</li>
</ol>
<center>
<img src="/2023/06/10/da/problem/uncertainty.png" width="50%">
</center>
<p>上图是贝叶斯推理处理回归任务的一个简单示例，其中蓝色的点表示训练数据，红色的实线表示预测的平均值，橙色部分表示预测的方差（也是预测结果的不确定度）。可以看到，训练数据稀疏处预测不确定度较大，而训练数据密集处预测的不确定度较小。如果是一般的优化方法，则只有红线部分，没有橙色的预测不确定度部分（给定输入数据，一个模型只能给出一个预测结果）。</p>
<h3 id="采样问题">采样问题</h3>
<blockquote>
<p>如何计算贝叶斯推理<span class="math inline">\((5)\)</span>?</p>
</blockquote>
<p>贝叶斯推理的目标计算式为：</p>
<p><span class="math display">\[
p(d_{test}|\mathcal{D}_{train}) = \mathbb{E}_{x \sim
p(x|\mathcal{D}_{train})}\left[p(d_{test}|x)\right],
\]</span></p>
<p>其中模型参数的后验分布 <span class="math inline">\(p(x|\mathcal{D}_{train})\)</span> 与似然函数<span class="math inline">\(p(d_{test}|x)\)</span>都是由神经网络建模的，意味着我们无法直接得到这个期望式的闭解（积分无法直接计算）。一个可行的替代方案是将<code>求期望</code>近似为<code>采样+求平均</code>：</p>
<p><span class="math display">\[
p(d_{test}|\mathcal{D}_{train}) \approx
\frac{1}{M}\sum^M_{i=1}p(d_{test}|x_i), \quad where \quad x_i\sim
p(x|\mathcal{D}_{train}).
\]</span></p>
<p>显然<span class="math inline">\(p(d_{test}|x_i)\)</span>部分是可以计算的，这就是神经网络的<code>forward</code>过程，所以剩余的问题是如何从分布<span class="math inline">\(p(x|\mathcal{D}_{train})\)</span>采样得到样本点<span class="math inline">\(x_i\)</span>。这部分的难点在哪里呢？首先我们并不知道完整的概率密度函数<span class="math inline">\(p(x|\mathcal{D}_{train})\)</span>，我们只能拿到一个正比的结论：</p>
<p><span class="math display">\[
p(x| \mathcal{D}_{train} ) \propto \prod^N_{j=1}{p( d_{train, j}
|x)}p(x),
\]</span></p>
<p>这是因为边缘归一化参数 <span class="math inline">\(p(\mathcal{D}_{train})\)</span>
无法计算。其次，在只能拿到概率密度（和其梯度）的条件下，如何对这个分布采样？简单的高斯分布、beta分布等可以非常方便地调用现有的api得到样本点，但是
<span class="math inline">\(p(x| \mathcal{D}_{train} )\)</span>
显然不属于这种情况。</p>
<ul>
<li>可以通过求积分的方式采样吗？No，<span class="math inline">\(p(x|
\mathcal{D}_{train})\)</span>形式复杂无法求积分</li>
<li>可以使用<code>拒绝采样</code>算法求解吗？No，我们无法得知<span class="math inline">\(p(x| \mathcal{D}_{train})\)</span>的最大值</li>
<li>可以使用<code>重要性采样</code>算法求解吗？理论上是可以的，但是我们不知道分布<span class="math inline">\(p(x|
\mathcal{D}_{train})\)</span>的“形状”，无法构建合理的参考分布</li>
</ul>
<p>此时再回顾分布近似问题的定义：</p>
<p><span class="math display">\[
\min_{\mu} \mathcal{F}(\mu, \nu),
\]</span></p>
<p>我们实际上想得到一个粒子点集合（或者说离散经验分布）<span class="math inline">\(\tilde{\mu} =
\{x_i\}^M_{i=1}\)</span>，使其尽可能接近目标分布 <span class="math inline">\(\nu=p(x|
\mathcal{D}_{train})\)</span>（先忽略度量<span class="math inline">\(\mathcal{F}\)</span>的具体定义）。关于该问题，通常有如下三种求解方案（这些方法都需要计算<span class="math inline">\(p(x|
\mathcal{D}_{train})\)</span>的梯度，不过这一项是很好求解，一般可以通过反向传播计算）：</p>
<ul>
<li>马尔科夫链蒙特卡洛（MCMC），尤其是2011年后发展的动力学MCMC方法</li>
<li>基于模型的变分推理（VI）</li>
<li>粒子变分推理（ParVI）</li>
</ul>
<blockquote>
<ul>
<li>Bayesian Learning via Stochastic Gradient Langevin Dynamics</li>
<li>Auto-Encoding Variational Bayes</li>
<li>Stein Variational Gradient Descent: A General Purpose Bayesian
Inference Algorithm</li>
</ul>
</blockquote>
<p>其中：</p>
<ul>
<li>VI参数化一个简单分布（比如高斯分布的均值和方差），将原优化变量<span class="math inline">\(\mu\)</span>改为优化高斯分布的均值和方差，得以在实向量空间求解。在优化结束后，从这个简单的高斯分布抽取样本构建样本点集<span class="math inline">\(\tilde{\mu} = \{x_i\}^M_{i=1}\)</span>。</li>
<li>由于简单分布结构上通常不足够灵活，VI方法的近似结果通常比较“粗糙”。ParVI则直接建模粒子点集<span class="math inline">\(\tilde{\mu} =
\{x_i\}^M_{i=1}\)</span>中每个粒子的位置，然后直接同时优化所有粒子点的位置，使粒子群近似目标分布。</li>
<li>MCMC有三种形态：
<ul>
<li>随机初始化一个粒子点，然后该粒子点按照某个规律做随机运动，搜集粒子点的运动轨迹，该轨迹的粒子点集合就是目标分布的采样结果</li>
<li>随机初始化一群粒子点，这群粒子点按照某个规律做随机运动，迭代若干次后的粒子群状态即为目标分布的采样结果</li>
<li>第一种和第二种形态可以混合使用</li>
</ul></li>
</ul>
<blockquote>
<p>这里不详细讲三种算法的详细方案，可以参考上述论文或后续章节</p>
</blockquote>
<p>三种算法的特性分别为：</p>
<ul>
<li>VI基于优化的方法，收敛快，扩展性好（优化结束后可生成任意多的样本点），但是近似精度低。</li>
<li>ParVI同样基于优化方法，收敛快，近似精度高（粒子数量少的情况下通常远高于MCMC和VI），但是扩展性低（优化结束后无法再增加粒子点），单次迭代的计算复杂度高。</li>
<li>MCMC基于随机游走，收敛慢，扩展性好，近似精度随着粒子点数量的增加而增加，但在少量粒子情况下精度差（随机性太高）。</li>
</ul>
<center>
<img src="/2023/06/10/da/problem/parvi.png" width="95%">
</center>
<p>上图是一个ParVI迭代过程（从左至右）的典型示例（二维分布近似），我将粒子点标上了颜色，可以更清楚观察到粒子点的移动轨迹。背景的等高线表示目标分布的概率密度。</p>
<h3 id="生成模型">生成模型</h3>
<p>采样问题的设定中，目标分布<span class="math inline">\(\nu\)</span>是连续分布，对空间中的点<span class="math inline">\(x\)</span>可计算该处的概率密度函数<span class="math inline">\(\nu(x)\)</span>。而分布近似问题的另一种设定，目标分布为离散经验分布（用<span class="math inline">\(\tilde{\nu}\)</span>表示），此时目标分布的概率密度函数不可知，<span class="math inline">\(\tilde{\nu}\)</span>表示为<span class="math inline">\(N\)</span>个粒子点<span class="math inline">\(\{y_j\}^N_{j=1}\)</span>（有权重的情况下表示为<span class="math inline">\(\{(\beta_j,y_j)\}^N_{j=1}\)</span>），此时分布近似问题可看作以度量<span class="math inline">\(\mathcal{F}\)</span>为损失函数的拟合问题。</p>
<blockquote>
<p>分布近似问题与生成模型的联系是什么？</p>
</blockquote>
<p>以CV的生成模型为例，这些模型的目标是将随机的高斯噪声样本<span class="math inline">\(z\)</span>转化为图像<span class="math inline">\(\tilde{x}\)</span>，并且希望生成的图像<span class="math inline">\(\tilde{x}\)</span>接近真实图像<span class="math inline">\(x\)</span>。我们假设模型参数可以用符号<span class="math inline">\(\theta\)</span>表示，由网络参数化的“生成”分布可表示为<span class="math inline">\(p_\theta(x)\)</span>。总而言之，这些符号的意义如下：</p>
<ul>
<li><span class="math inline">\(\tilde{\nu}\)</span>表示目标分布，也就是训练的数据集，包含了多张真实图像。</li>
<li>随机高维噪声用<span class="math inline">\(z\)</span>表示。</li>
<li>神经网络的参数用<span class="math inline">\(\theta\)</span>表示，随机噪声<span class="math inline">\(z\)</span>可以通过神经网络转化为图像<span class="math inline">\(\tilde{x}=f_\theta(z)\)</span>。</li>
<li>由神经网络参数化的“生成分布”表示为<span class="math inline">\(p_\theta(x)\)</span>。</li>
</ul>
<blockquote>
<p>这里有一点符号混淆的问题是我们同时用<span class="math inline">\(x\)</span>表示实向量空间中的点以及真实图像，而用<span class="math inline">\(\tilde{x}\)</span>仅表示生成图像，但是应该比较好理解。</p>
</blockquote>
<blockquote>
<p>什么是参数化的“生成分布”呢？这里生成网络的作用是将高斯分布的噪声转化为图像，也就是说生成的图像实际上来自于某个未知的分布，由于大多数的神经网络（除了特殊设计的Flow
Model）是不可逆的，这个生成的分布无法计算概率密度函数，我们能得到的仅仅只有通过神经网络生成的“真实”图像样本。这里仅仅是把这个未知分布用数学符号<span class="math inline">\(p_\theta(x)\)</span>表示。</p>
</blockquote>
<p>接下来我们以VAE（变分自编码器）为例，介绍如何推导出VAE的优化目标。首先我们从分布近似问题的定义出发：</p>
<p><span class="math display">\[
\min_\theta{\mathcal{F}(p_\theta, \tilde{\nu})},
\]</span></p>
<p>上式表达的意思是：优化一个神经网络，使得网络参数化的分布<span class="math inline">\(p_\theta\)</span>接近目标分布<span class="math inline">\(\tilde{\nu}\)</span>（真实图像），进而使生成的图像接近真实图像。具体来说，VAE使用的度量<span class="math inline">\(\mathcal{F}\)</span>是<span class="math inline">\(KL\)</span>散度：</p>
<p><span class="math display">\[
\begin{align*}
&amp;\min_\theta{KL(\tilde{\nu} \| p_\theta)}\\
\Rightarrow &amp; \min_\theta
\int{\log{\frac{\tilde{\nu}}{p_\theta}}\mathrm{d}\tilde{\nu}} \\
\Rightarrow &amp; \max_\theta \int{\log{p_\theta}\mathrm{d}\tilde{\nu}}
\\
\Rightarrow &amp; \max_\theta \mathbb{E}_{x \sim
\tilde{\nu}}\left[\log{p_\theta(x)}\right],
\end{align*}
\]</span></p>
<p>上式就是VAE（也包括了早期的diffusion
model）使用的“极大似然”损失函数（优化目标）。</p>
<p>关于<span class="math inline">\(\log{p_\theta(x)}\)</span>，进一步可得：</p>
<p><span class="math display">\[
\begin{align*}
p_\theta(x) &amp;= \int{p_\theta(x|z)p(z)\mathrm{d}z} \\
&amp;= \frac{p_\theta(x|z)p(z)}{p_\theta(z|x)}\\
&amp;= \frac{p_\theta(x|z)p(z)q_\phi(z|x)}{p_\theta(z|x)q_\phi(z|x)}
\end{align*}
\]</span></p>
<blockquote>
<p>这里的<span class="math inline">\(q_\phi(z|x)\)</span>和<span class="math inline">\(p_\theta(z|x)\)</span>的含义相似，可以看作是不同神经网络参数化的后验分布。熟悉VAE的读者应该可以看出<span class="math inline">\(p_\theta(z|x)\)</span>是“未知的”编码器，<span class="math inline">\(q_\phi(z|x)\)</span>是实际使用的近似编码器，<span class="math inline">\(p_\theta(x|z)\)</span>是解码器。</p>
</blockquote>
<p>对等式两边求对数可得：</p>
<p><span class="math display">\[
\begin{align*}
\log{p_\theta(x)} &amp;=
\log{\frac{p_\theta(x|z)p(z)q_\phi(z|x)}{p_\theta(z|x)q_\phi(z|x)}} \\
&amp;= \log{\frac{p_\theta(x|z)p(z)}{q_\phi(z|x)}} -
\log{\frac{p_\theta(z|x)}{q_\phi(z|x)}}
\end{align*}
\]</span></p>
<p>等式两边乘<span class="math inline">\(q_\phi(z|x)\)</span>然后对<span class="math inline">\(z\)</span>求积分可得：</p>
<p><span class="math display">\[
\begin{align*}
\int{q_\phi(z|x)\log{p_\theta(x)}\mathrm{d}z}
&amp;=  \int{q_\phi(z|x)\log{\frac{p_\theta(x|z)p(z)}{q_\phi(z|x)}}\mathrm{d}z}
- \int{q_\phi(z|x)\log{\frac{p_\theta(z|x)}{q_\phi(z|x)}}\mathrm{d}z}
\end{align*}
\]</span></p>
<p>注意到<span class="math inline">\(\log{p_\theta(x)}\)</span>与积分变量<span class="math inline">\(z\)</span>无关，可得：</p>
<p><span class="math display">\[
\begin{align*}
\log{p_\theta(x)}
&amp;=  \int{q_\phi(z|x)\log{\frac{p_\theta(x|z)p(z)}{q_\phi(z|x)}}\mathrm{d}z}
- \int{q_\phi(z|x)\log{\frac{p_\theta(z|x)}{q_\phi(z|x)}}\mathrm{d}z}\\
&amp;= \underbrace{\mathbb{E}_{z\sim
q_\phi(z|x)}\left[\log{\frac{p_\theta(x|z)p(z)}{q_\phi(z|x)}}\right]}_{ELBO}
+ KL\left(q_\phi(z|x) \| p_\theta(z|x)\right),
\end{align*}
\]</span></p>
<p>上述过程就是ELBO（Evidence Lower
BOund）的数学推导，注意到KL散度的结果一定是非负的，因此有：</p>
<p><span class="math display">\[
\log{p_\theta(x)} \geq \mathbb{E}_{z\sim
q_\phi(z|x)}\left[\log{p_\theta(x|z)p(z) - \log{q_\phi(z|x)}}\right]
\]</span></p>
<p>整理公式，可得如下的优化目标：</p>
<p><span class="math display">\[
\max_{\theta, \phi} \mathbb{E}_{x\sim \tilde{\nu}}\left[
\mathbb{E}_{z\sim q_\phi(z|x)}\left[\log{p_\theta(x|z)p(z) -
\log{q_\phi(z|x)}}\right] \right]
\]</span></p>
<p>实际上，VAE和diffusion
model的优化目标就是上式，VAE用神经网络建模编码器<span class="math inline">\(q_\phi(z|x)\)</span>，用一个神经网络建模解码器<span class="math inline">\(p_\theta(x|z)\)</span>，diffusion用随机过程建模编码器，并且将解码过程建模为多步神经网络的forward过程。</p>
<blockquote>
<p>那么生成对抗网络GAN也可以表达成类似的形式吗？</p>
</blockquote>
<p>在2014年发表的最初的GAN论文中可以找到作者对GAN优化目标的数学理解，实际上，GAN最小化参数分布<span class="math inline">\(p_\theta\)</span>和目标分布<span class="math inline">\(\tilde{\nu}\)</span>之间的<span class="math inline">\(JS\)</span>散度（假设判别器以训练到最优）：</p>
<p><span class="math display">\[
\min_\theta KL\left(\tilde{\nu} \left\| \frac{\tilde{\nu} + p_\theta}{2}
\right. \right) + KL\left(p_\theta \left\| \frac{\tilde{\nu} +
p_\theta}{2} \right. \right).
\]</span></p>
<p>除了<code>Vanilla GAN</code>，还有<code>Wasserstein-GAN</code>，<code>MMD-GAN</code>，<code>Sinkhorn-GAN</code>等等，这些都可以用：</p>
<p><span class="math display">\[
\min_\mu{\mathcal{F}(\mu, \tilde{\nu})}
\]</span></p>
<p>表达，区别在于生成分布<span class="math inline">\(\mu\)</span>的定义形式和使用的度量<span class="math inline">\(\mathcal{F}\)</span>不同。</p>
<blockquote>
<ul>
<li>Wasserstein GAN</li>
<li>MMD GAN: Towards Deeper Understanding of Moment Matching
Network</li>
<li>Learning Generative Models with Sinkhorn Divergences</li>
</ul>
</blockquote>
<p>最后，粒子变分推理法可以直接计算离散经验分布<span class="math inline">\(\tilde{\mu}\)</span>，使其近似真实图像分布<span class="math inline">\(\tilde{\nu}\)</span>。这个过程可以没有网络，ParVI可以直接计算出每个噪声平滑地变换为图像的过程，当然也可以用神经网络拟合这个过程，这也是2022年末很多diffusion模型的做法，这里我们不讨论详细的数学过程。</p>
<blockquote>
<p>代码可参考：https://github.com/lzj-isee/dpvi_discrete。</p>
</blockquote>
<center>
<img src="/2023/06/10/da/problem/flow.png" width="90%">
</center>
<p>上图是ParVI方法在cifar10数据集上的一个简单实验结果。</p>

    </div>

    
    
    

      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" rel="tag"># 机器学习</a>
              <a href="/tags/%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%AD%A6%E4%B9%A0/" rel="tag"># 贝叶斯学习</a>
              <a href="/tags/%E5%88%86%E5%B8%83%E8%BF%91%E4%BC%BC/" rel="tag"># 分布近似</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item"></div>
      <div class="post-nav-item">
    <a href="/2023/07/01/collections/llm_collections/" rel="next" title="基于知识库的LLM问答系统参考资料">
      基于知识库的LLM问答系统参考资料 <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%88%86%E5%B8%83%E8%BF%91%E4%BC%BC%E9%97%AE%E9%A2%98"><span class="nav-number">1.</span> <span class="nav-text">分布近似问题</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%97%AE%E9%A2%98%E5%AE%9A%E4%B9%89"><span class="nav-number">1.1.</span> <span class="nav-text">问题定义</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%AD%A6%E4%B9%A0"><span class="nav-number">1.2.</span> <span class="nav-text">贝叶斯学习</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%87%87%E6%A0%B7%E9%97%AE%E9%A2%98"><span class="nav-number">1.3.</span> <span class="nav-text">采样问题</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B"><span class="nav-number">1.4.</span> <span class="nav-text">生成模型</span></a></li></ol></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">Li zhijian</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">6</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">11</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="mailto:lizhijian@zju.edu.cn" title="E-Mail → mailto:lizhijian@zju.edu.cn" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2023</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Li zhijian</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a>
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  

</body>
</html>
